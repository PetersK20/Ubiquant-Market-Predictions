# -*- coding: utf-8 -*-
"""17th-place-solution.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rqUHT0_JdTFzaaPlK3ELspHPqyPy42eD
"""

import gc
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import plotly.express as px

from pathlib import Path
from tqdm import tqdm
import lightgbm as lgb
from sklearn.model_selection import *z
import lightgbm as lgb

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from torch.autograd import Variable 
from tqdm import tqdm
from torch.utils.data.sampler import WeightedRandomSampler
import lightgbm as lgb
from lightgbm import early_stopping, print_evaluation
import ubiquant
env = ubiquant.make_env()

#Determine the features we want to utilize
features = ['f_0', 'f_1',
       'f_2', 'f_3', 'f_4', 'f_5', 'f_6', 'f_7', 'f_8', 'f_9', 'f_10',
       'f_11', 'f_12', 'f_13', 'f_14', 'f_15', 'f_16', 'f_17', 'f_18',
       'f_19', 'f_20', 'f_21', 'f_22', 'f_23', 'f_24', 'f_25', 'f_26',
       'f_27', 'f_28', 'f_29', 'f_30', 'f_31', 'f_32', 'f_33', 'f_34',
       'f_35', 'f_36', 'f_37', 'f_38', 'f_39', 'f_40', 'f_41', 'f_42',
       'f_43', 'f_44', 'f_45', 'f_46', 'f_47', 'f_48', 'f_49', 'f_50',
       'f_51', 'f_52', 'f_53', 'f_54', 'f_55', 'f_56', 'f_57', 'f_58',
       'f_59', 'f_60', 'f_61', 'f_62', 'f_63', 'f_64', 'f_65', 'f_66',
       'f_67', 'f_68', 'f_69', 'f_70', 'f_71', 'f_72', 'f_73', 'f_74',
       'f_75', 'f_76', 'f_77', 'f_78', 'f_79', 'f_80', 'f_81', 'f_82',
       'f_83', 'f_84', 'f_85', 'f_86', 'f_87', 'f_88', 'f_89', 'f_90',
       'f_91', 'f_92', 'f_93', 'f_94', 'f_95', 'f_96', 'f_97', 'f_98',
       'f_99', 'f_100', 'f_101', 'f_102', 'f_103', 'f_104', 'f_105',
       'f_106', 'f_107', 'f_108', 'f_109', 'f_110', 'f_111', 'f_112',
       'f_113', 'f_114', 'f_115', 'f_116', 'f_117', 'f_118', 'f_119',
       'f_120', 'f_121', 'f_122', 'f_123', 'f_124', 'f_125', 'f_126',
       'f_127', 'f_128', 'f_129', 'f_130', 'f_131', 'f_132', 'f_133',
       'f_134', 'f_135', 'f_136', 'f_137', 'f_138', 'f_139', 'f_140',
       'f_141', 'f_142', 'f_143', 'f_144', 'f_145', 'f_146', 'f_147',
       'f_148', 'f_149', 'f_150', 'f_151', 'f_152', 'f_153', 'f_154',
       'f_155', 'f_156', 'f_157', 'f_158', 'f_159', 'f_160', 'f_161',
       'f_162', 'f_163', 'f_164', 'f_165', 'f_166', 'f_167', 'f_168',
       'f_169', 'f_170', 'f_171', 'f_172', 'f_173', 'f_174', 'f_175',
       'f_176', 'f_177', 'f_178', 'f_179', 'f_180', 'f_181', 'f_182',
       'f_183', 'f_184', 'f_185', 'f_186', 'f_187', 'f_188', 'f_189',
       'f_190', 'f_191', 'f_192', 'f_193', 'f_194', 'f_195', 'f_196',
       'f_197', 'f_198', 'f_199', 'f_200', 'f_201', 'f_202', 'f_203',
       'f_204', 'f_205', 'f_206', 'f_207', 'f_208', 'f_209', 'f_210',
       'f_211', 'f_212', 'f_213', 'f_214', 'f_215', 'f_216', 'f_217',
       'f_218', 'f_219', 'f_220', 'f_221', 'f_222', 'f_223', 'f_224',
       'f_225', 'f_226', 'f_227', 'f_228', 'f_229', 'f_230', 'f_231',
       'f_232', 'f_233', 'f_234', 'f_235', 'f_236', 'f_237', 'f_238',
       'f_239', 'f_240', 'f_241', 'f_242', 'f_243', 'f_244', 'f_245',
       'f_246', 'f_247', 'f_248', 'f_249', 'f_250', 'f_251', 'f_252',
       'f_253', 'f_254', 'f_255', 'f_256', 'f_257', 'f_258', 'f_259',
       'f_260', 'f_261', 'f_262', 'f_263', 'f_264', 'f_265', 'f_266',
       'f_267', 'f_268', 'f_269', 'f_270', 'f_271', 'f_272', 'f_273',
       'f_274', 'f_275', 'f_276', 'f_277', 'f_278', 'f_279', 'f_280',
       'f_281', 'f_282', 'f_283', 'f_284', 'f_285', 'f_286', 'f_287',
       'f_288', 'f_289', 'f_290', 'f_291', 'f_292', 'f_293', 'f_294',
       'f_295', 'f_296', 'f_297', 'f_298', 'f_299',"Missing"]

for i in ['f_170','f_272','f_182','f_124','f_200','f_175','f_102','f_153','f_108','f_8','f_145', 'f_225', 'f_241', 'f_63', 'f_229', 'f_246', 'f_41', 'f_66', 'f_142', 'f_150', 'f_99', 'f_74', 'f_62', 'f_271']:
    features.remove(i)

# Import train data
dtypes = {
    'row_id': 'str',
    'time_id': 'uint16',
    'investment_id': 'uint16',
    'target': 'float32',
}

for i in range(300):
    dtypes[f'f_{i}'] = 'float32'
    
trainSet = pd.read_csv('../input/ubiquant-market-prediction/train.csv',dtype=dtypes)

path = '../input/ubiquant-market-prediction/supplemental_train.csv'
sup_train = pd.read_csv(path,usecols=list(dtypes.keys()),dtype=dtypes)
trainSet = pd.concat([trainSet[trainSet["time_id"]>=849],sup_train],axis=0)
del sup_train
gc.collect()

trainSet["TimeIdPrev"] = trainSet.groupby("investment_id")["time_id"].shift(1)
trainSet["Missing"] = 0
trainSet.loc[trainSet["TimeIdPrev"]+1 != trainSet["time_id"] ,"Missing"] = 1
trainSet["target"] = trainSet.groupby("time_id")["target"].transform(lambda x: (x-x.mean())/x.std())

trainSet.replace([np.inf, -np.inf], np.nan, inplace=True)
trainSet.fillna(0, inplace=True)

missingIndex = 1
indexMap = {}
for index, row in trainSet[trainSet["time_id"] == trainSet["time_id"].max()].iterrows():
    indexMap[row.investment_id] = missingIndex
missingIndex+=1

from torch.utils.data.sampler import WeightedRandomSampler
targetVar = "target"
train = torch.as_tensor(trainSet.loc[:,features].to_numpy(), dtype=torch.float32)
trainLabel = torch.as_tensor(trainSet.loc[:,targetVar].to_numpy(), dtype=torch.float32)
timeIdTrain = torch.tensor(trainSet.loc[:,"time_id"].to_numpy().astype(np.float32)).long()
trainDataset = TensorDataset(train,trainLabel,timeIdTrain)
weights = (np.array(trainSet.loc[:,"time_id"]-650)) / (trainSet.loc[:,"time_id"].max()-650)
sampler = WeightedRandomSampler(weights, len(weights))
dataloaderTrain = DataLoader(trainDataset,batch_size=128, sampler=sampler, shuffle=False)
dataloaderTrain2 = DataLoader(trainDataset,batch_size=1000,sampler=sampler, shuffle=False)

del trainSet
gc.collect()

numFeat = len(features)-1
class MLP(nn.Module):
    def __init__(self, input_size):
        super(MLP, self).__init__()
        self.bn = nn.BatchNorm1d(numFeat)
        self.Input = nn.Sequential(nn.Dropout(.1),nn.Linear(input_size,1000),nn.BatchNorm1d(1000),nn.Dropout(.5),nn.ReLU(),nn.Linear(1000,512),nn.Dropout(.25),nn.ReLU(),nn.Linear(512,1))
    def forward(self, data, isTrain=False):
        x = torch.cat([self.bn(data[:,0:numFeat]),data[:,numFeat:]],1)
        x = self.Input(x)
        return x
cos = nn.CosineSimilarity(dim=0, eps=1e-6)
def cosLoss(pred,label):
    return -cos(pred,label)
def variance(preds, labels):
    x = preds - preds.mean()
    y = labels - labels.mean()
    return (-(x*y)).mean()

epoch = 21
device="cpu"
input_size = len(features)
criterion = nn.MSELoss()
criterion2 = variance
criterion3 = nn.L1Loss()
lr = .00002
varLR = .000006/10

model1a = []
model1b = []

MLPModel = MLP(input_size).to(device)
optimizer = optim.Adam(params = MLPModel.parameters(), lr=.00002)
for e in range(epoch):

    for data, labels, time_id in tqdm(dataloaderTrain):
        optimizer.zero_grad()
        data = data.to(device)
        labels = labels.to(device)
        preds = MLPModel(data, isTrain=True)

        loss = criterion(preds.reshape(-1), labels)

        loss.backward()
        optimizer.step()
    if e==10 or e==15 or e==20:
        model1a.append(MLPModel.state_dict())

    for g in optimizer.param_groups:
        lr = g["lr"]
        g["lr"]=varLR

    for g in optimizer.param_groups:
        g["lr"]=lr*.85
        
    if e==10 or e==15 or e==20:
        model1b.append(MLPModel.state_dict())
model1 = []
for checkpoint in model1a:
    MLPModel = MLP(input_size).to(device)
    MLPModel.load_state_dict(checkpoint)
    model1.append(MLPModel)
for checkpoint in model1b:
    MLPModel = MLP(input_size).to(device)
    MLPModel.load_state_dict(checkpoint)
    model1.append(MLPModel)
gc.collect()

for model in model1:
    model.eval()
features.pop()

iter_test = env.iter_test()
for (test_df, sample_prediction_df) in iter_test:
    test_df.replace([np.inf, -np.inf], np.nan, inplace=True)
    test_df.fillna(0, inplace=True)
    try:
        missing = np.zeros(test_df.shape[0])
        for index in range(test_df.shape[0]):
            row = test_df.iloc[index]
            lastTimeId = indexMap.get(row.investment_id,-1)
            diffLastTime = (missingIndex - lastTimeId)-1
            if (diffLastTime) and (lastTimeId!=-1):
                missing[index] = 1

            indexMap[row.investment_id] = missingIndex

        data = np.concatenate([test_df[features].to_numpy(),(missing.reshape(-1,1))],1)

        with torch.no_grad():

            preds1 = torch.zeros(data.shape[0])
            data = torch.as_tensor(data, dtype=torch.float32).to(device)
            for model in model1:
                preds1 = preds1 + model(data).cpu()/6

            sample_prediction_df['target'] = preds1.numpy()

            sample_prediction_df.replace([np.inf, -np.inf], np.nan, inplace=True)
            sample_prediction_df.fillna(0, inplace=True)

            env.predict(sample_prediction_df)
    except Exception:
        sample_prediction_df['target'] = np.random.randn(len(sample_prediction_df))

        env.predict(sample_prediction_df)
    missingIndex+=1